---
title: [NLP] 한국어 임베딩(2019) 2장 정리
layout: post
date: '2020-01-29 18:00:00'
excerpt: 벡터와 자연어: 숫자가 의미를 갖는 과정
tag:
- nlp
- tf-idf
- BoW
- n-gram
category: nlp
comments: true
---

---

※ 이 글은 [한국어 임베딩(2019)](http://acornpub.co.kr/book/korean-embedding) 책을 개인적으로 정리하고 잡다한 것들을 추가한 글입니다.<br>원문과 똑같은 부분은... 이해를 못해서 그대로 쓴 부분입니다.

※ 책은 [깃허브](https://github.com/ratsgo/embedding)에 있는 소스들을 사용하고 있습니다. 셸을 사용하는 경우 여기 있는 파일들을 돌리는 겁니다.

<br>

## 1. 자연어 계산

자연어는 사람이 사용하는 언어를 말한다.<br>컴퓨터는 자연어를 사람처럼 이해하지 못한다.<br>단지 계산기일 뿐이기 때문이다.

컴퓨터가 자연어를 처리하기 위해서는 계산할 수 있도록 바꿔줘야 한다.<br>자연어를 계산되는 숫자들로 바꿔주는 과정이 **`임베딩(embedding)`**이다.<br>임베딩 결과 산출된 숫자들의 나열이 **`벡터(vector)`**다.

임베딩 방식을 정리하면 다음과 같다.

| 구분        | bag of words 가정         | 언어 모델                 | 분포 가정             |
| ----------- | ------------------------- | ------------------------- | --------------------- |
| 내용        | 어떤 단어가 (많이) 쓰였나 | 단어가 어떤 순서로 쓰였나 | 뭔 단어가 같이 쓰였나 |
| 대표 통계량 | TF-IDF                    |                           | PMI                   |
| 대표 모델   | Deep Averaging Network    | ELMo, GPT                 | Word2Vec              |

<br>

---

## 2. 어떤 단어가 (많이) 쓰였나

**(1) 백오브워즈(bag of words,  BoW)**

백(bag)은 [멀티세트](https://en.wikipedia.org/wiki/Multiset)(multiset)를 말한다.<br>멀티세트는 일반적인 세트(set)와 달리 각각 요소에 대해 복수의 인스턴스를 허용하는 개념이다.

집합 $$\{a,a,b,b\}$$ 를 예로 들자.<br>세트는 $$\{a,b\}$$ 형식으로 표현할 수 있다.<br>멀티세트는 $$\{a^2,b^2\}$$ 처럼 표현할 수 있다.<br>**원소마다 몇 개 있는지** 볼 수 있다는 점이 멀티세트의 차별성이다.

파이썬 기준으로는 이렇게 보면 편할 것이다.

```python
# 세트
{a, b} # 일반적인 set와 같다.
# 멀티세트
{'a':2, 'b':2} # 일반적인 dictionary와 같다.
```

세트와 멀티세트는 순서를 고려하지 않는다.<br>파이썬에서 set, dict가 순서를 고려하지 않듯이 말이다.

백오브워즈 임베딩은<br>`저자가 생각한 주제가 문서에서의 단어 사용에 녹아있다`는 가정이 깔려있다.<br>**주제**가 비슷한 문서는<br>**단어 등장 여부** 역시 비슷할 것이고,<br>**백오브워즈 임베딩** 역시 유사하다는 입장이다.

다음 백오브워즈 임베딩 표에 의하면<br>`사랑 손님과 어머니`, `삼포 가는 길` 문서 관련성이 상대적으로 높다.

| 구분   | 운수 좋은 날 | 사랑 손님과 어머니 | 삼포 가는 길 |
| ------ | ------------ | ------------------ | ------------ |
| 기차   | 2            | 10                 | 7            |
| 막걸리 | 1            | 0                  | 0            |
| 선술집 | 0            | 0                  | 0            |

백오브워즈 임베딩은 간단하긴 하지만, **정보 검색** 분야에서 이걸 가지고 코사인 유사도를 구해 정보를 노출시키는 경우가 많다고 한다.



**(2) TF-IDF(Term Frequency-Inverse Document Frequency)**

단어 빈도 및 등장여부를 문서 주제파악을 위해 그대로 쓰는 것은 큰 단점이 있다.<br>많이 등장한 단어가 의미있는 단어가 아니기 때문이다.

[지프의 법칙](https://ko.wikipedia.org/wiki/지프의_법칙)(Zipf's law)를 예로 들겠다.<br>영어를 기준으로 보면, 단어의 사용빈도는 the, of, and 순이다.<br>우리말로 치면 조사, 대명사가 제일 많이 등장하는 꼴이다.<br>조사나 대명사는 그 자체가 주제를 갖기 어려운 단어다.<br>빈도만으로 문서의 주제를 파악하기 어려운 이유다.

[TF-IDF](https://ko.wikipedia.org/wiki/Tf-idf)는 그런 단점을 보완한 방법이다.<br>단어 빈도(Term Frequency)와  문서 빈도의 역행렬(Inverse Document Frequency)를 이용한다.<br>TF와 DF를 나눈다는 개념이며, DF를 분모로 사용하기 때문에 IDF라 한다.<br>기본적인 수식은 다음과 같다.
$$
TF-IDF(w)=TF(w) \times log(\frac{N}{DF(w)})
$$
$$w$$는 단어(word)의 빈도다. $$N$$은 전체 문서의 갯수다.<br>여러가지 TF-IDF가 존재하는데, 위 기본 수식에 가중치를 더한 것들이다.<br>난 저 식의 $$N$$을 $$N+1$$로 바꿀 것이다.<br>특정 단어가 모든 문서에 나온다면 IDF값이 $$log1$$이 되고,<br>$$log1$$은 0이기 때문에 모든 수치를 0으로 만들어버리기 때문이다.<br>문서가 많으면 괜찮을 수 있으나, 예시문에서는 표본이 적어서 의미있는 단어도 날려버린다.

위 백오브워즈 임베딩을 바탕으로 TF_IDF를 구해보자.<br>$$N$$은 3, $$w$$는 아까 백오브워즈 임베딩에 나온 숫자들이다.

| 구분   | 운수 좋은 날       | 사랑 손님과 어머니 | 삼포 가는 길      |
| ------ | ------------------ | ------------------ | ----------------- |
| 기차   | 0.5753641449035617 | 2.8768207245178083 | 2.013774507162466 |
| 막걸리 | 1.3862943611198906 | 0                  | 0                 |
| 선술집 | 0                  | 0                  | 0                 |

백오브워즈 기준으로 `운수 좋은 날`의 `기차`는 2건, `막걸리`는 1건이었다.<br>TF-IDF를 실행한 결과, `운수 좋은 날`에서는 1건만 있는 `막걸리`가 2건만 있는 `기차`보다 높은 가중치를 갖고 있다.<br>`운수 좋은 날`의 `기차` 값을 구한 식은 다음과 같다.
$$
TF(w) \times log(\frac{N+1}{DF(w)})=2 \times log(\frac{3+1}{3})\fallingdotseq0.575
$$
패키지 없이 야매로 구한 것이니, TF-IDF가 이런 것이다 수준으로 이해하고 넘어가도록 한다.



**(3) Deep Averaging Network**

Deep Averaging Network(Iyyer et al., 2015)는 백오브워즈 가정의 뉴럴 네트워크 버전이다.<br>

<figure>
    <a href="https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/05/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-05-10-13.29.52.png?w=366&ssl=1"><center><img src="https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/05/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-05-10-13.29.52.png?w=366&ssl=1"><center></a>
    <figcaption><center><b>Deep Averaging Network</b></center></figcaption>
</figure>

위 사례는 `Predator is a masterpiece`라는 문장의 임베딩을 4개의 토큰으로 구분하였다.<br>일반 뉴럴 네트워크와 다른 점은, sequence 데이터로 취급하지 않고 빈도만 따진다는 점이다.<br>이 점에서 백오브워즈와 비슷하다고 한다.

빈도만으로 문장 임베딩을 한 뒤 해당 문서가 어떤 범주인지 분류하는 모델이다.<br>간단한 구조임에도 불구하고 성능이 좋아서 현업에도 자주 쓰인다고 한다.

<br>

---

## 3. 단어가 어떤 순서로 쓰였나

**(1) 통계 기반 언어 모델**

언어 모델(language model)이란 단어 시퀀스(sequence, 연속적 순서)에 **확률**을 부여하는 모델이다.<br>백오브워즈와 반대로, 단어 등장 순서를 따진다.

통계 기반 언어 모델은 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습한다.<br>잘 학습되면 어떤 문장이 그럴 듯한지(확률이 높은지) 알 수 있다.<br>아래와 같이 말이다.

| 문장                             | 확률 |
| -------------------------------- | ---- |
| 진이는 이 책을 세 번 읽었다.     | 0.47 |
| 이 책이 진이한테 세 번을 읽혔다. | 0.23 |
| 세 번이 진이한테 이 책을 읽혔다. | 0.07 |

그러나 통계상 한 번도 집계되지 않은(빈도가 없는) 경우는 계산할 수 없다.<br>이를 위해 **n-gram**을 사용한다.

n-gram은 n개 단어를 뜻한다. 예를 들면,<br>`난폭, 운전`은 2-gram(bigram), `누명, 을, 쓰다`는 3-gram(trigram), `바람, 잘, 날, 없다`는 4-gram이다.<br>

| 표현                    | 빈도 |
| ----------------------- | ---- |
| 이제                    | 112  |
| 식사를                  | 56   |
| 할                      | 341  |
| 시간이다                | 28   |
| 이제 식사를             | 9    |
| 식사를 할               | 7    |
| 할 시간이다             | 2    |
| 이제 식사를 할 시간이다 | 0    |

위 표를 통해 n-gram 사용 예시를 들고자 한다.<br>`이제 식사를 할` 다음에 `시간이다`가 나타날 확률은 **조건부확률**을 활용해 **최대우도추정법**(Maximum Likelihood Estimation, MLE)로 유도하면 다음과 같다.
$$
P(시간이다|이제,식사를,할)=\frac{Freq(이제,식사를,할,시간이다)}{Freq(이제,식사를,할)}=0
$$
Freq는 해당 표현 순서가 등장한 빈도(frequency)를 뜻한다.<br>`이제 식사를 할 시간이다`가 0건이기 때문에 확률은 0이 된다.<br>딱히 틀린 문장이 아님에도 이런 문장이 나올 확률이 없다고 계산하는 것이다.

여기서 n-gram 모델을 쓰면 이런 문제를 일부 해결할 수 있다.<br>직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사하는 것이다.<br>[마르코프 가정](https://ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84)에 따르면, **"한 상태의 확률은 그 직전상태에만 의존한다."**<br>(註. 위키에서는 "과거와 현재 상태가 주어졌을 때의 미래 상태의 조건부 확률 분포가 과거 상태와는 독립적으로 현재 상태에 의해서만 결정된다."고 작성. 확실히 책의 설명이 훨씬 매끄럽다.)<br>예시에서는 `시간이다`가 등장할 확률은 `시간이다`의 직전상태, 즉 `할`에만 의존하는 것이다.
$$
P(시간이다|이제,식사를,할)\approx\frac{Freq(할,시간이다)}{Freq(할)}=\frac{2}{341}
$$
그렇다면 2-gram 모델 에서 `이제 식사를 할 시간이다`라는 시퀀스가 등장할 확률은 어떻게 되는가?<br>위와 같은 방식으로 구한 확률들을 전부 곱하는 것이다.<br>아래와 같이 말이다.
$$
P(이제,식사를,할,시간이다)\\\approx P(이제)\times P(식사를|이제)\times P(할|식사를)\times P(시간이다|할) \\= \frac{112}{|V|}\frac{9}{112}\frac{7}{56}\frac{2}{341}
$$
바이그램 모델을 일반화한 수식은 다음과 같다.
$$
P(w_{n}|w_{n-1})=\frac{Freq(w_{n-1},w_n)}{Freq(w_{n-1})}\\P(w^n_1)=P(w_1,w_2,\cdots,w_n)= \prod^n_{k=1}P(w_k|w_{k-1})
$$
(註. 책은 위와 같이 나왔지만, $$w_0$$이 없으니까. [여기](https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/) 식을 참조해서 수치를 조정하는 것이 맞아보인다.<br>n-gram 모형의 경우 수식도 나왔으니 참고하자.)

이런 n-gram에도 아직 문제가 남아있다.<br>데이터에 한 번도 등장하지 않는 n-gram이 존재하면 아까와 마찬가지로 확률이 0이 된다.<br>이 현상은 데이터엔 등장하지 않는 희소한 단어가 있기 때문이다.

`또바기`는 '언제나 한결같이 꼭 그렇게' 라는 뜻의 한국어 부사다.<br>거의 쓰지 않는 단어이며, 나도 본 적이 없다.<br>만약 사용했다고 해도, 바이그램 모델은 `또바기`라는 단어 앞의 단어와 같이 쓴 적이 있어야 확률을 계산할 수 있다.<br>`그 아이는 또바기 인사를 잘한다`라는 문장에서 `또바기` 앞에 `아이는`이 나온 적이 없다면,<br>$$P(또바기|아이는)=0$$ 이 되고, 바이그램 모델에서 이 문장이 자연스럽다고 여길 확률 역시 0이 된다.

이를 해결할 방법은 2가지가 있다.<br>백오프(back-off), 스무딩(smoothing)이다.

백오프는 n-gram 등장빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식이다.<br>아까 빈도가 0인 문장 `이제 식사를 할 시간이다`는 4-gram이다.<br>이를 백오프를 통해 2-gram인 `할 시간이다`를 이용해 빈도를 보정하는 법은 아래와 같다.<br>($$\alpha,\beta$$ 는 빈도 차이를 보정하는 파라미터.)
$$
Freq(이제,식사를,할,시간이다)\approx\beta+\alpha Freq(할,시간이다)
$$
스무딩은 모든 표현에 k만큼 빈도를 더하는 것이다.<br>이러면 `이제 식사를 할 시간이다`는 0+k만큼 빈도를 갖는 것이다.<br>Add-k 스무딩이라고 하며, k가 1일 경우 특별히 **라플라스 스무딩**이라 한다.<br>이를 통해 빈도가 0인 케이스에도 일부 확률을 부여하게 된다.



**(2) 뉴럴 네트워크 기반 언어모델**

단어들의 빈도를 세서 학습하는 통계 기반 언어모델과 달리,<br>뉴럴 네트워크를 통해 입,출력 관계 자체로 확률모델을 만들 수 있다.<br>단어 시퀀스를 가지고 다음 단어를 예측하는 학습으로 ELMo, GPT 등이 있다.

시계열성을 띄는 시퀀스 대신 앞, 뒤 단어를 전부 고려하는 양방향 학습이 있다.<br>**마스크 언어 모델**이다. BERT가 대표적이다.

여기에 대해서는 차후 설명할 예정이다.

<br>

---

## 4. 뭔 단어가 같이 쓰였나

**(1) 분포 가정**

자연어 처리에서 **분포(distribution)**란 특정 범위, <br>즉 윈도우(window)내에 동시 등장하는 이웃 단어 또는 문맥(context)의 집합을 가리킨다.

단어의 포는 단어가 문장 내에 나타나는 위치, 주변에 자주 등장하는 단어 여부에 따라 달라진다.<br><u>어떤 단어의 쌍(pair)이 비슷한 문맥에서 자주 등장한다면 그 의미 또한 유사할 것</u>이라는 게 분포 가정의 전제다.

**`분포 가정`**은 "단어의 의미는 곧 그 언어에서의 활용이다"라는 언어학자 비트겐슈타인의 철학에 기반해 있다.<br>다시 말해 모국어 화자들의 문맥상 단어 사용을 통해 그 단어의 의미를 밝힐 수 있다는 것이다.

`빨래`, `세탁`을 예시로 들어보자.

"`청소`와 `빨래`가 취미입니다."<br>"뜨거운 `물`로 `빨래`를 합니다."

"찬 `물`로 옷을 `세탁`한다."<br>"`세탁`, `청소`는 귀찮은 일이다."

`빨래`, `세탁`은 타깃 단어(target word), `청소`,`물` 등은 타깃 단어 주의에 등장한 문맥 단어(context word)다.<br>주변 문맥 단어들이 비슷한 것으로 보아, 타깃 단어들끼리 관계가 있을 것이라 짐작할 수 있다.<br>이를 통해 `빨래`, `세탁`은 서로 비슷한 부류의 행동으로 파악할 수 있다.

하지만 직접적인 연관성이 있어 보이진 않는다.<br><u>분포 정보가 곧 의미라는 분포 가정 자체에 의문이 제기될 수 있다는 소리다.</u><br>이를 해소하기 위해 언어학적 관점에서 분포 가정이 의미를 갖는 경우를 살펴보고자 한다.



**(2) 분포와 의미_1: 형태소**

언어학에서 형태소(morpheme)란 의미를 갖는 최소 단위를 말한다.<br>일반적으로, 단어를 쪼갰을 때 의미가 사라지면 쪼개기 전 단어를 형태소로 본다.<br>`밥`을 `ㅂ`, `압`으로 분리하면 의미가 사라지기 때문에 `밥`을 형태소로 보는 것이다.

언어학적으로 형태소를 분석하는 방법은 조금 다르다.<br>계열 관계(paradigmatic relation)가 대표적인 기준이다.<br>말뭉치(corpus)들을 대거 분석한 결과, `밥`이 `빵`으로 대신 쓰일 수 있다면,<br>이를 근거로 `밥`에 형태소 자격을 부여한다.

위와 같이, 언어학자들 역시 특정 타깃 단어 주변의 문맥정보를 바탕으로 형태소를 확인하고 있음을 알 수 있다.<br>말뭉치의 분포 정보와 형태소가 밀접한 관계를 이루고 있다는 것이다.



**(3) 분포와 의미_3: 품사**

언어학자들이 제시하는 품사 분류 기준은<br>**의미(meaning)**, **형태(form)**, **기능(function)** 등 세 가지다.

의미(meaning)는 실제 품사를 분류할 때 결정적 분류 기준이 될 수 없다.<br>`공부하다`, `공부`의 경우를 예시로 들어보자.<br>한국어 화자들은 `공부하다`는 동사, `공부`는 명사로 분류할 것이다.<br>하지만 `공부`라는 단어에 '움직임'이 없어서 동사로 구분하지 않는 것일까? 딱 잘라 말하기 어렵다.<br>의미가 결정적 분류기준이 아닌 이유다.

형태(form) 역시 마찬가지다.<br>`철수가 있다`와 `철수! 뭐해!` 에서 전자의 `철수`는 명사, 후자의 `철수`는 감탄사로 쓰였다.

**기능(function)**이 바로 언어학자들이 꼽는 가장 중요한 품사 분류기준이다.<br>해당 단어가 문장 내에서 점하는 역할에 초점을 맞추는 것이다.<br>그런데 언어에서는 <u>단어의 **분포**가 **기능**과 매우 밀접하다고 한다.</u> <br>한국어 품사 분류의 일반적인 기준을 간단히 살펴봄으로써 마치겠다.

```markdown
체언(명사): 관형사가 그 앞에 올 수 있고 조사가 그 뒤에 올 수 있음
용언(동사/형용사): 부사가 그 앞에 올 수 있고 선어말어미가 그 뒤에 올 수 있고 어말어미가 그 뒤에 와야함
관형사: 명사가 그 뒤에 와야 함
(이하 생략)
```

이러한 언어의 특성 덕분에 분포 정보를 함축한 **벡터**에 **의미**를 내재시킬 수 있다.



**(4) 점별 상호 정보량**

**`점별 상호 정보량(Pointwise Mutual Infomation, PMI)`**은 두 확률변수(random variable) 사이의 상관성을 계량하는 단위다.<br>예를 들면, 단어 A가 등장하면 단어 B 등장할 확률이 얼마나 되는지 확인하는 것이다.<br>A가 나왔을 때 B가 자주 나타날 수록 PMI값이 커지고, B가 안 나올 수록 PMI값이 작아진다.<br>수식으로 표현하면 아래와 같다.
$$
PMI(A,B)=log\frac{P(A,B)}{P(A)\times P(B)}
$$
전체 빈도가 1,000회, `빨래`가 20회, `속옷`이 15회, `빨래`와 `속옷`이 동시 등장한 경우가 10회인 경우를 가정하자.<br>이 때 PMI값을 구하면 아래와 같다.
$$
PMI(빨래,속옷)=log\frac{P(빨래,속옷)}{P(빨래)\times P(속옷)}=log\frac{\frac{10}{1000}}{\frac{20}{1000}\times\frac{15}{1000}}
$$


**(5) Word2Vec**

Word2Vec은 구글 연구팀이 2013년 발표한 임베딩 기법이며, 분포 가정의 가장 대표적인 모델이다.<br>CBOW 모델은 문맥 단어들로 타깃 단어 하나를 맞추는 과정에서 학습된다.<br>Skip-gram 모델은 타깃 단어 하나로 문맥 단어를 예측하는 과정에서 학습된다.<br>둘 다 특정 타깃 단어 주변의 문맥, 즉 <u>분포 정보를 임베딩에 함축한다.</u><br>PMI 행렬과도 깊은 연관이 있다.

<figure>
    <a href="\posts_image\nlp\ch2_word2vec.jpg"><img src="\posts_image\nlp\ch2_word2vec.jpg"></a>
    <figcaption><center><b>CBOW와 Skip-gram 모델</b></center></figcaption>
</figure>

<br>

---

## 5. 주요 내용

- 임베딩에 자연어의 통계적 패턴(statistical pattern) 정보를 주면 자연어의 의미(semantic)를 함축할 수 있다.
- 백오브워즈 가정에서는 어떤 단어의 등장 여부 혹은 빈도 정보를 중시한다. 순서 정보는 무시한다.
- 언어 모델은 위와 반대로 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여한다.
- 분포 가정에서는 문장에서 어떤 단어가 같이 쓰였는지를 중시한다.
- 백오브워즈 가정, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 다른 각도에서 분석하며, 상호 보완적이다.

---

<br>

#### 참고

- [한국어 임베딩(2019)](http://acornpub.co.kr/book/korean-embedding)



